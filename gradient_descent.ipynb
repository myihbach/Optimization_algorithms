{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "gradient_descent.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPofH30FnuR6MwLlcKnaLyZ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/myihbach/Optimization_algorithms/blob/main/gradient_descent.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pZ7O1IaWp0WM"
      },
      "source": [
        "import numpy as np\n",
        "import time\n",
        "import random as rd"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DNSGAnHdp58F"
      },
      "source": [
        "def J1(v):\n",
        "  N= len(v)\n",
        "  v0 = np.ones(N)\n",
        "  return [np.dot(v-v0,v-v0),2*(v-v0)]\n",
        "\n",
        "def J2(v):\n",
        "  N= len(v)\n",
        "  v0 = np.arange(1,N+1)\n",
        "  return [np.dot(v-v0,v-v0),2*(v-v0)]\n",
        "\n",
        "def cost(v):\n",
        "  return J1(v)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GbCBoLFqrZl5"
      },
      "source": [
        "### **Fixed step Gradient descent Algorithm**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6pd1WQ4vrBBc",
        "outputId": "e1525ee5-01d2-4cc3-c0f4-006e4a687902"
      },
      "source": [
        "def gradient_descent_fixed_step():\n",
        "  t = time.time()\n",
        "  N=10 #dimension \n",
        "  eps = 10**-10\n",
        "  Kmax = 1500 \n",
        "  eta =0.055\n",
        "  nbr_iteration=0\n",
        "  u0 = np.zeros(N)\n",
        "  grad = cost(u0)[1]\n",
        "  while np.linalg.norm(grad) > eps and nbr_iteration <= Kmax :\n",
        "    u0 = u0 - eta * grad\n",
        "    grad = cost(u0)[1]\n",
        "    nbr_iteration += 1\n",
        "  \n",
        "  print(u0)\n",
        "  print(nbr_iteration)\n",
        "  print('time : ' , time.time() - t)\n",
        "\n",
        "gradient_descent_fixed_step()\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
            "214\n",
            "time :  0.004116535186767578\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c72-D_dX1nu9"
      },
      "source": [
        "### **Gradient descent optimal step**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wxtqMY741q-d",
        "outputId": "2bfea263-cd56-447f-cb25-c2389c8c8cef"
      },
      "source": [
        "def gradient_descent_optimal_step(u0):\n",
        "  t = time.time()\n",
        "  N=10 #dimension \n",
        "  eps = 10**-10\n",
        "  Kmax = 1500 \n",
        "  eta = 0.05\n",
        "  nbr_iteration=0\n",
        "  grad = cost(u0)[1]\n",
        "  while np.linalg.norm(grad) > eps and nbr_iteration < Kmax :\n",
        "    g_g = np.dot(grad,grad)\n",
        "    eta = (g_g * eta*eta ) /( 2 * ( cost(u0- eta* grad)[0] - cost(u0)[0] + eta* g_g ))\n",
        "    u0 = u0 - eta * grad\n",
        "    grad = cost(u0)[1]\n",
        "    nbr_iteration += 1\n",
        "  \n",
        "  print(u0)\n",
        "  print(nbr_iteration)\n",
        "  print('time : ' , time.time() - t)\n",
        "\n",
        "gradient_descent_optimal_step(np.zeros(5))\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1. 1. 1. 1. 1.]\n",
            "1\n",
            "time :  0.0005884170532226562\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VeomfgjqDbp2"
      },
      "source": [
        "\n",
        "> Etudier les méthodes GF et GO sur le cas de la fonction $J_H (x, y) = (x^2 +y−2)^2 +(y^2 −2x+1)^2$ . On prendra comme guess initial $u0 = (0, 0)$ puis $u0 = (1.5, −1.5)$. Que constatez-vous ? Comment valider ce constat ?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ALQ8UoR_owEk"
      },
      "source": [
        "def J3 (v):\n",
        "  N = len (v)  \n",
        "  J = np.diagflat( np.ones (n-1), 1)\n",
        "  A = 2*np.eye(N) - J - J.T\n",
        "  f = np.ones(len(v)) \n",
        "  return 0.5*np.dot(Av,v) - np.dot(f,v), Av - f , A\n",
        "\n",
        "def J4 (v):\n",
        "  n = len (v) \n",
        "  J1 = np.diagflat( np.ones (n-1), 1)\n",
        "  J2 = np.diagflat( np.ones (n-2), 2)\n",
        "  A = 4*np.eye(n) - J1 - J1.T - J2 - J2.T \n",
        "  f = np.ones(n) \n",
        "  return 0.5*np.dot(A*v,v) - np.dot(f,v), A*v - f , A\n",
        "\n",
        "def cost(v):\n",
        "  return J4(v)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 147
        },
        "id": "X4qO3wkidHng",
        "outputId": "e097c0b7-71d6-4006-a4ec-27ebb4cbfd5f"
      },
      "source": [
        "\"\"\"def gradient_conjuge():\n",
        "  t = time.time()\n",
        "  N=10 #dimension \n",
        "  eps = 10**-10\n",
        "  u0=np.ones(N)\n",
        "  nbr_iteration=0\n",
        "  d0 = grad = cost(u0)[1]\n",
        "  while np.linalg.norm(grad) > eps :\n",
        "    Ad = cost(u0)[2] * d0\n",
        "    rho =  np.dot(grad,d0) / np.dot(Ad,d0)\n",
        "    u0 = u0 - rho * d0 \n",
        "    \n",
        "    grad = cost(u0)[1]\n",
        "    bk = - np.dot(grad,Ad) / np.dot(Ad,d0)\n",
        "    d0 = grad + bk * d0\n",
        "    nbr_iteration += 1\n",
        "  \n",
        "  print(u0)\n",
        "  print(nbr_iteration)\n",
        "  print('time : ' , time.time() - t)\n",
        "\n",
        "gradient_conjuge()\"\"\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"def gradient_conjuge():\\n  t = time.time()\\n  N=10 #dimension \\n  eps = 10**-10\\n  u0=np.ones(N)\\n  nbr_iteration=0\\n  d0 = grad = cost(u0)[1]\\n  while np.linalg.norm(grad) > eps :\\n    Ad = cost(u0)[2] * d0\\n    rho =  np.dot(grad,d0) / np.dot(Ad,d0)\\n    u0 = u0 - rho * d0 \\n    \\n    grad = cost(u0)[1]\\n    bk = - np.dot(grad,Ad) / np.dot(Ad,d0)\\n    d0 = grad + bk * d0\\n    nbr_iteration += 1\\n  \\n  print(u0)\\n  print(nbr_iteration)\\n  print('time : ' , time.time() - t)\\n\\ngradient_conjuge()\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    }
  ]
}